import typer
from typing_extensions import Annotated
import os
import json
import re
from glob import glob

from ailoy import Runtime, VectorStore, Agent, APIModel
from tc_gen.document_parser import extract_text_from_html
from tc_gen.tc_generator import create_tc_excel, read_tc_excel

app = typer.Typer()

# --- Constants ---
OUTPUT_DIR = "tc_gen/output" # Relative to the project root
KNOWLEDGE_BASE_FILENAME = os.path.join(OUTPUT_DIR, "knowledge_base.jsonl")

PARSE_SRS_SYSTEM_MESSAGE = "ë‹¹ì‹ ì€ SRS ë¬¸ì„œë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ íŒŒì‹±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ìš”êµ¬ì‚¬í•­ì€ ID, ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…, ìœ í˜•(FR/NFR)ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."

UPDATE_SYSTEM_MESSAGE = """ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³ , ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ìˆ˜ì •ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
{
  "id": "TC-{{ìš”êµ¬ì‚¬í•­ID}}-001",
  "title": "{{í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œëª©}}",
  "preconditions": [
    "{{ì‚¬ì „ ì¡°ê±´ 1}}",
    "{{ì‚¬ì „ ì¡°ê±´ 2}}"
  ],
  "test_steps": [
    {
      "step": 1,
      "action": "{{ë™ì‘ 1}}",
      "test_data": "{{í…ŒìŠ¤íŠ¸ ë°ì´í„° 1}}",
      "expected_result": "{{ì˜ˆìƒ ê²°ê³¼ 1}}"
    }
  ],
  "notes": "{{ì°¸ê³  ì‚¬í•­}}"
}
"""

_current_llm_model_id = "gemini-2.5-pro" # Module-level variable to track preferred model

def _call_llm_with_fallback(rt: Runtime, api_key: str, prompt: str, system_message_for_agent: str = None) -> str:
    global _current_llm_model_id  # Declare intent to modify global variable

    for model_fallback_attempt in range(2):  # Loop for model fallback (pro -> flash)
        for retry_503_attempt in range(5):  # Loop for 503 retries (up to 5 times)
            try:
                agent = Agent(rt, APIModel(id=_current_llm_model_id, api_key=api_key), system_message=system_message_for_agent)
                full_response_content = ""
                response_iterator = agent.query(prompt)

                for resp in response_iterator:
                    if resp.type == "output_text":
                        full_response_content += resp.content
                return full_response_content  # Success
            except Exception as e:
                error_message = str(e)
                if "overloaded" in error_message or "503" in error_message:
                    if retry_503_attempt < 4:  # Check if more retries are left (0, 1, 2, 3)
                        print(f"âš ï¸ Model is overloaded. Retrying attempt {retry_503_attempt + 2}/5 for model {_current_llm_model_id}...")
                        continue  # This will retry the inner loop
                    else:
                        # 503 retries exhausted for this model, break to trigger model fallback
                        print(f"âŒ Model is still overloaded after 5 attempts for {_current_llm_model_id}.")
                        break  # break from inner loop
                elif "Quota exceeded" in error_message or "429" in error_message:
                    # Quota error, break inner loop immediately to trigger model fallback
                    print(f"âš ï¸ Quota exceeded for {_current_llm_model_id}.")
                    break  # break from inner loop
                else:
                    raise  # Other errors, fail immediately
        
        # This block is reached if the inner loop was broken (not returned from).
        # This means we either had a quota error or 503 retries were exhausted.
        # Time to fall back to the next model.
        if _current_llm_model_id == "gemini-2.5-pro":
            print(f"âš ï¸ Switching to gemini-2.5-flash and retrying...")
            _current_llm_model_id = "gemini-2.5-flash"
            # The outer loop will continue with the new model
        else:
            # We were already on the fallback model, and it failed.
            print(f"âŒ Fallback model also failed. No further fallback available.")
            raise  # Re-raise the last exception
            
    raise Exception("Failed to get LLM response after all attempts and fallbacks.")


DEFAULT_TC_EXCEL_FILENAME = "test_cases.xlsx"
DEFAULT_SRS_JSONL_FILENAME = "srs.jsonl"
DEFAULT_SYSTEM_MESSAGE_FILE = os.path.join(OUTPUT_DIR, "system_message.md")


@app.command()
def parse(
    input_dir: Annotated[str, typer.Argument(help="The path to the directory containing HTML documents (e.g., Confluence export).")]
):
    global _current_llm_model_id # Declare intent to modify global variable
    """Parses all HTML documents, creates knowledge_base.jsonl, and generates a system_message.md summary."""
    print(f"ğŸ” Starting parsing of directory: {input_dir}")

    all_knowledge_chunks = []

    html_files = glob(os.path.join(input_dir, "**/*.html"), recursive=True)
    if not html_files:
        print(f"âŒ Error: No HTML files found in {input_dir}")
        raise typer.Exit(code=1)

    for html_file_path in html_files:
        base_filename = os.path.basename(html_file_path)
        print(f"  Processing {base_filename}...")
        try:
            with open(html_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            general_chunks = extract_text_from_html(content, source_file=base_filename)
            all_knowledge_chunks.extend(general_chunks)
            print(f"    Extracted {len(general_chunks)} general chunks.")

        except Exception as e:
            print(f"âŒ Error processing {base_filename}: {e}")

    try:
        with open(KNOWLEDGE_BASE_FILENAME, 'w', encoding='utf-8') as f:
            for chunk in all_knowledge_chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
        print(f"\nğŸ“„ Successfully created knowledge base: {KNOWLEDGE_BASE_FILENAME} ({len(all_knowledge_chunks)} chunks).")
    except Exception as e:
        print(f"âŒ Error writing knowledge_base.jsonl: {e}")
        raise typer.Exit(code=1)

    # Automatically generate system_message.md from the knowledge base
    print("\nğŸ¤– Generating system_message.md from knowledge base...")
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ Error: GEMINI_API_KEY not set. Skipping system_message.md generation.")
    else:
        rt = Runtime()
        try:
            knowledge_base_content = "\n".join([chunk['content'] for chunk in all_knowledge_chunks])
            
            max_kb_length = 128
            if len(knowledge_base_content) > max_kb_length*1024:
                print(f"âš ï¸ Knowledge base content is very large ({len(knowledge_base_content)} chars). Truncating to {max_kb_length} for prompt.")
                knowledge_base_content = knowledge_base_content[:max_kb_length*1024]

            system_message_prompt = f"""
            ë‹¹ì‹ ì€ ì œê³µëœ ê¸°ìˆ  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬, í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± AIë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ Knowledge Base ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ `system_message.md` íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
            ì¶œë ¥ì€ ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì™„ì „í•œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

            ë‹¤ìŒ êµ¬ì¡°ì™€ ì§€ì¹¨ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”:

            # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€

            ## 1. ì œí’ˆ ê°œìš”
            (Knowledge Baseë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê°„ê²°í•œ ê°œìš”ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ì£¼ìš” ëª©ì ê³¼ ëŒ€ìƒ ì‚¬ìš©ìë¥¼ ì„œìˆ í•˜ê³ , í•µì‹¬ ê¸°ëŠ¥ì€ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ëª©ë¡ìœ¼ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.)

            ## 2. ì£¼ìš” ìš©ì–´ ì •ì˜
            (Knowledge Baseì—ì„œ ì¤‘ìš”í•œ ê¸°ìˆ  ìš©ì–´ì™€ ì•½ì–´ë¥¼ ì‹ë³„í•˜ê³ , ì•„ë˜ì˜ ë§ˆí¬ë‹¤ìš´ ì •ì˜ ëª©ë¡ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•´ì£¼ì„¸ìš”.)
            **ì˜ˆì‹œ í˜•ì‹:**
            - **V2X**: Vehicle-to-Everythingì˜ ì•½ìë¡œ, ì°¨ëŸ‰ì´ ì£¼ë³€ì˜ ëª¨ë“  ê²ƒê³¼ í†µì‹ í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            - **PKI**: Public Key Infrastructureì˜ ì•½ìë¡œ, ê³µê°œ í‚¤ ì•”í˜¸ ë°©ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë³´ì•ˆ ì¸í”„ë¼ì…ë‹ˆë‹¤.

            ## 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ê°€ì´ë“œë¼ì¸
            (ì•„ë˜ ê°€ì´ë“œë¼ì¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.)
            - ë‹¹ì‹ ì€ **ISTQB Advanced Level ìê²©ì¦ì„ ì†Œì§€í•œ QA ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
            - ê° ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ Positive ì‹œë‚˜ë¦¬ì˜¤ì™€ Negative ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê°ê° 1ê°œì”© ìƒì„±í•©ë‹ˆë‹¤.
            - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œëª©ì€ ì‹œë‚˜ë¦¬ì˜¤ì˜ ëª©ì ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê°„ê²°í•˜ê³  ì„œìˆ ì ì¸ ë¬¸êµ¬ì—¬ì•¼ í•©ë‹ˆë‹¤. ì œëª©ì— 'Positive', 'Negative'ì™€ ê°™ì€ ë¶„ë¥˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            - ì œê³µëœ Knowledge Base ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í˜„ì‹¤ì ì´ê³  íš¨ê³¼ì ì¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            - í…ŒìŠ¤íŠ¸ ë‹¨ê³„ëŠ” ì‚¬ìš©ìì˜ í–‰ë™, í…ŒìŠ¤íŠ¸ ë°ì´í„°, ê·¸ë¦¬ê³  ì˜ˆìƒë˜ëŠ” ì‹œìŠ¤í…œì˜ ë°˜ì‘ì„ ëª…í™•í•˜ê²Œ ê¸°ìˆ í•´ì•¼ í•©ë‹ˆë‹¤.

            ---
            ## 4. ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ (SRS)
            (ì´ ì„¹ì…˜ì—ëŠ” ë‹¤ìŒ ë¬¸êµ¬ë¥¼ ê·¸ëŒ€ë¡œ í¬í•¨í•´ì£¼ì„¸ìš”: "(ì´ ì„¹ì…˜ì€ í”„ë¡œê·¸ë¨ì— ì˜í•´ SRS íŒŒì¼ì˜ ë‚´ìš©ê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” ì˜ì—­ì…ë‹ˆë‹¤.)")

            ---
            ì‚¬ìš©í•  Knowledge Base ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
            --- Knowledge Base ---
            {knowledge_base_content}
            --- END Knowledge Base ---

            ì´ì œ `system_message.md` íŒŒì¼ì˜ ì™„ì „í•˜ê³  ìµœì¢…ì ì¸ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
            """

            generator_agent_system_message = "ë‹¹ì‹ ì€ ì œê³µëœ ê¸°ìˆ  ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, ë‹¤ë¥¸ AIë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            
            _current_llm_model_id = "gemini-2.5-flash"
            print(f"ğŸ¤– Requesting LLM ({_current_llm_model_id}) to generate system_message.md...")
            generated_system_message = _call_llm_with_fallback(
                rt=rt,
                api_key=api_key,
                prompt=system_message_prompt,
                system_message_for_agent=generator_agent_system_message
            )
            _current_llm_model_id = "gemini-2.5-pro"
            
            # Clean up potential markdown code blocks from the response
            match = re.search(r"```(markdown)?\s*([\s\S]*?)\s*```", generated_system_message, re.IGNORECASE)
            if match:
                generated_system_message = match.group(2)

            with open(DEFAULT_SYSTEM_MESSAGE_FILE, 'w', encoding='utf-8') as f:
                f.write(generated_system_message)
            
            print(f"ğŸ“„ Successfully generated system message file: {DEFAULT_SYSTEM_MESSAGE_FILE}")

        except Exception as e:
            print(f"âŒ An error occurred during system_message.md generation: {e}")
        finally:
            rt.stop()

    print("âœ¨ Parsing completed successfully!")


@app.command(name="parse-srs")
def parse_srs(
    srs_md_file: Annotated[str, typer.Argument(help="The path to the manually created SRS Markdown file.")],
    output_file: Annotated[str, typer.Option(help="The filename for the output .jsonl file.")] = os.path.join(OUTPUT_DIR, DEFAULT_SRS_JSONL_FILENAME),
):
    """Parses a Markdown SRS file into a structured .jsonl format using LLM."""
    print(f"ğŸ” Parsing SRS Markdown file: {srs_md_file} using LLM...")
    
    try:
        with open(srs_md_file, 'r', encoding='utf-8') as f:
            srs_md_content = f.read()
    except FileNotFoundError:
        print(f"âŒ Error: SRS Markdown file {srs_md_file} not found.")
        raise typer.Exit(code=1)

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ Error: GEMINI_API_KEY environment variable not set.")
        raise typer.Exit(code=1)

    rt = Runtime()
    try:
        system_message = PARSE_SRS_SYSTEM_MESSAGE
        user_prompt = f"""
        ë‹¤ìŒì€ Markdown í˜•ì‹ì˜ SRS ë¬¸ì„œì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œ ê° ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­(FR)ê³¼ ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­(NFR)ì„ ì¶”ì¶œí•˜ì—¬ JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”. ê° ì¤„ì€ í•˜ë‚˜ì˜ JSON ê°ì²´ì—¬ì•¼ í•˜ë©°, ë‹¤ìŒ í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
        - ID: ìš”êµ¬ì‚¬í•­ì˜ ê³ ìœ  ì‹ë³„ì (ì˜ˆ: PV25-FR-01)
        - ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…: ìš”êµ¬ì‚¬í•­ì˜ ì „ì²´ ë‚´ìš©
        - type: 'FR' ë˜ëŠ” 'NFR'

        --- SRS Markdown ë¬¸ì„œ ---
        {srs_md_content}

        --- ì¶œë ¥ í˜•ì‹ ---
        ê° ìš”êµ¬ì‚¬í•­ì€ í•œ ì¤„ì˜ JSON ê°ì²´ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ:
        {{"ID": "PV25-FR-01", "ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…": "ì¸ì¦ì„œë¥¼ ë°œê¸‰í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤", "type": "FR"}}
        {{"ID": "PV25-NFR-01", "ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…": "ì¸ì¦ì„œ ë°œê¸‰ ì„±ëŠ¥: p95 latency â‰¤ 40 ms", "type": "NFR"}}
        """
        
        print(f"ğŸ¤– Requesting LLM ({_current_llm_model_id}) to parse SRS Markdown...")
        full_response_content = _call_llm_with_fallback(
            rt=rt,
            api_key=api_key,
            prompt=user_prompt,
            system_message_for_agent=system_message
        )
        
        # LLM ì‘ë‹µì„ .jsonl íŒŒì¼ë¡œ ì €ì¥ (ê° ì¤„ì´ ìœ íš¨í•œ JSONì¸ì§€ ê²€ì¦)
        lines = full_response_content.strip().split('\n')
        valid_json_lines = []
        for line in lines:
            line = line.strip()
            if line.startswith('{') and line.endswith('}'):
                try:
                    json.loads(line) # Validate if it's a valid JSON
                    valid_json_lines.append(line)
                except json.JSONDecodeError:
                    print(f"âš ï¸ Warning: Skipping invalid JSON line in LLM response: {line}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(valid_json_lines))
        
        print(f"ğŸ“„ Successfully parsed SRS Markdown to {output_file} ({len(valid_json_lines)} items).")

    except Exception as e:
        print(f"âŒ An error occurred during LLM-powered SRS parsing: {e}")
        raise typer.Exit(code=1)
    finally:
        rt.stop()

    print("âœ¨ SRS Markdown parsing completed successfully!")


@app.command()
def generate(
    srs_jsonl_file: Annotated[str, typer.Argument(help="The path to the parsed SRS .jsonl file.")],
    knowledge_base_file: Annotated[str, typer.Argument(help="The path to the knowledge base .jsonl file.")],
    output: Annotated[str, typer.Option(help="The filename for the output Excel file.")] = DEFAULT_TC_EXCEL_FILENAME,
    use_rag: Annotated[bool, typer.Option(help="Whether to use RAG (VectorStore) for context retrieval. Defaults to False.")] = False,
    system_message_file: Annotated[str, typer.Option(help="The path to the Markdown file containing the base system message.")] = DEFAULT_SYSTEM_MESSAGE_FILE,
):
    """Generate test cases for all SRS items in the parsed file."""
    print(f"Generating test cases from SRS: {srs_jsonl_file}")
    print(f"Using knowledge base: {knowledge_base_file}")
    print(f"Output will be saved to: {output}")

    srs_items_for_tc_gen = []
    try:
        with open(srs_jsonl_file, 'r', encoding='utf-8') as f:
            srs_items_for_tc_gen = [json.loads(line) for line in f]
        print(f"ğŸ“„ Loaded {len(srs_items_for_tc_gen)} SRS items from {srs_jsonl_file}")
    except FileNotFoundError:
        print(f"âŒ Error: SRS .jsonl file {srs_jsonl_file} not found.")
        raise typer.Exit(code=1)
    except Exception as e:
        print(f"âŒ Error reading SRS .jsonl file: {e}")
        raise typer.Exit(code=1)

    system_message_base = ""
    if os.path.exists(system_message_file):
        try:
            with open(system_message_file, 'r', encoding='utf-8') as f:
                system_message_base = f.read()
            print(f"ğŸ“„ Loaded base system message from: {system_message_file}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not read system message file {system_message_file}. Proceeding without it. Error: {e}")
    else:
        print(f"âš ï¸ Warning: System message file {system_message_file} not found. Proceeding without a base message.")

    srs_content_for_system_message = "\n"
    for item in srs_items_for_tc_gen:
        srs_content_for_system_message += f"ID: {item.get('ID')}\nìœ í˜•: {item.get('type')}\nìƒì„¸ ì„¤ëª…: {item.get('ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…')}\n---\n"

    final_system_message = f"{system_message_base}\n{srs_content_for_system_message}"

    knowledge_chunks = []
    try:
        with open(knowledge_base_file, 'r', encoding='utf-8') as f:
            knowledge_chunks = [json.loads(line) for line in f]
        print(f"ğŸ“„ Loaded {len(knowledge_chunks)} chunks from knowledge base.")
    except FileNotFoundError:
        print(f"âŒ Error: Knowledge base file {knowledge_base_file} not found.")
        raise typer.Exit(code=1)
    except Exception as e:
        print(f"âŒ Error reading knowledge base file: {e}")
        raise typer.Exit(code=1)

    rt = Runtime()
    vs = None
    try:
        if use_rag:
            with VectorStore(rt, "BAAI/bge-m3", "faiss") as initialized_vs:
                vs = initialized_vs
                print("ğŸ¤– Building in-memory VectorStore from knowledge base... (This may take a while)")
                all_insert_items = []
                for chunk in knowledge_chunks:
                    document = chunk.get('content', '')
                    metadata = {'source_file': chunk.get('source_file'), 'type': chunk.get('type')}
                    embedding = vs.embedding(document)
                    all_insert_items.append({
                        "embedding": embedding,
                        "document": document,
                        "metadata": metadata,
                    })

                if all_insert_items:
                    vs._runtime.call_method(
                        vs._component_state.vector_store_name,
                        "insert_many",
                        {"items": all_insert_items},
                    )
                print("âœ… VectorStore built.")
        else:
            print("âš ï¸ RAG is disabled. Using simple string matching for context retrieval.")

        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            print("âŒ Error: GEMINI_API_KEY environment variable not set.")
            raise typer.Exit(code=1)
        
        print("ğŸ§‘â€ğŸ’» Agent initialized with enhanced system message.")
        final_tcs_to_write = {}

        if os.path.exists(output):
            print(f"ğŸ“„ Loading existing test cases from {output}...")
            existing_tcs = read_tc_excel(output)
            for tc in existing_tcs:
                final_tcs_to_write[tc.get("id")] = tc
            print(f"ğŸ“„ Loaded {len(existing_tcs)} existing test cases.")
            max_tc_id = 0
            for tc_id_str in final_tcs_to_write.keys():
                match = re.match(r"TC-(\d+)", tc_id_str)
                if match:
                    max_tc_id = max(max_tc_id, int(match.group(1)))
            tc_id_counter = max_tc_id + 1
            print(f"Starting TC ID counter from: {tc_id_counter}")
        else:
            tc_id_counter = 1
            print("No existing test cases found. Starting TC ID counter from 1.")

        for srs_item in srs_items_for_tc_gen:
            related_existing_tcs = [
                tc for tc in final_tcs_to_write.values() 
                if tc.get("srs_id") == srs_item.get("ID")
            ]
            
            if related_existing_tcs:
                related_existing_tc_ids = {tc.get("id") for tc in related_existing_tcs}
                print(f"â© Skipping TC generation for {srs_item.get('ID')}. Already found TCs: {', '.join(related_existing_tc_ids)}")
                continue
            print(f"\nâœ¨ Generating TC for {srs_item.get('ID')}...")
            
            query_text = srs_item.get('ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª…')
            if query_text is None:
                query_text = srs_item.get('ìš”êµ¬ì‚¬í•­')
            context_str = ""

            if use_rag and vs is not None:
                retrieved_context = vs.retrieve(query_text, top_k=5)
                context_str = "\n".join([item.document for item in retrieved_context])
            else:
                matched_chunks = []
                for chunk in knowledge_chunks:
                    if query_text.lower() in chunk.get('content', '').lower():
                        matched_chunks.append(chunk.get('content', ''))
                
                if matched_chunks:
                    context_str = "\n".join(matched_chunks)
                    print(f"  - Found {len(matched_chunks)} matching chunks using simple string matching.")
                else:
                    print("  - No matching chunks found using simple string matching.")

            user_prompt = f"""
            ë‹¤ìŒ SRS ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì´ ìš”êµ¬ì‚¬í•­ì€ ì´ë¯¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì œê³µëœ ì „ì²´ SRSì˜ ì¼ë¶€ì…ë‹ˆë‹¤.
            
            --- í˜„ì¬ SRS ìš”êµ¬ì‚¬í•­ ---
            ID: {srs_item.get('ID')}
            ìœ í˜•: {srs_item.get('type')}
            ìƒì„¸ ì„¤ëª…: {query_text}
            
            --- ì¶”ê°€ ì°¸ê³  ìë£Œ (Knowledge Base) ---
            {context_str if context_str else "ì œê³µëœ ì°¸ê³  ìë£Œ ì—†ìŒ."}
            
            --- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ì§€ì‹œ ---
            ìœ„ ìš”êµ¬ì‚¬í•­ê³¼ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ, Positive ì‹œë‚˜ë¦¬ì˜¤ 1ê°œì™€ Negative ì‹œë‚˜ë¦¬ì˜¤ 1ê°œë¥¼ í¬í•¨í•˜ì—¬ ì´ 2ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œëª©ì€ í•„ìˆ˜ì´ë©°, í•´ë‹¹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ëª©ì ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê°„ê²°í•˜ê³  ì„œìˆ ì ì¸ ë¬¸êµ¬ì—¬ì•¼ í•©ë‹ˆë‹¤. ì œëª©ì— 'Positive', 'Negative'ì™€ ê°™ì€ ë¶„ë¥˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ê³ , ì œëª©ë§Œìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì¶©ë¶„íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, 'ì„±ê³µì ì¸ ì¸ì¦ì„œ ë°œê¸‰ ìš”ì²­' ë˜ëŠ” 'ì •ì±… ìœ„ë°˜ìœ¼ë¡œ ì¸í•œ ì¸ì¦ì„œ ë°œê¸‰ ìš”ì²­ ê±°ì ˆ'ê³¼ ê°™ì´ ì‘ì„±í•©ë‹ˆë‹¤. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” ë‹¤ìŒ JSON í˜•ì‹ì— ë§ì¶° ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤:
            
            ```json
            [
              {{
                "srs_id": "{srs_item.get('ID')}",
                "title": "{{í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œëª©}}",
                "preconditions": [
                  "{{ì‚¬ì „ ì¡°ê±´ 1}}",
                  "{{ì‚¬ì „ ì¡°ê±´ 2}}"
                ],
                "test_steps": [
                  {{
                    "step": 1,
                    "action": "{{ë™ì‘ 1}}",
                    "test_data": "{{í…ŒìŠ¤íŠ¸ ë°ì´í„° 1}}",
                    "expected_result": "{{ì˜ˆìƒ ê²°ê³¼ 1}}"
                  }},
                  {{
                    "step": 2,
                    "action": "{{ë™ì‘ 2}}",
                    "test_data": "{{í…ŒìŠ¤íŠ¸ ë°ì´í„° 2}}",
                    "expected_result": "{{ì˜ˆìƒ ê²°ê³¼ 2}}"
                  }}
                ],
                "notes": "{{ì°¸ê³  ì‚¬í•­}}"
              }}
            ]
            ```
            
            JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
            """

            print(f"ğŸ¤– Requesting LLM ({_current_llm_model_id}) to generate TC for {srs_item.get('ID')}...")
            full_response_content = _call_llm_with_fallback(
                rt=rt,
                api_key=api_key,
                prompt=user_prompt,
                system_message_for_agent=final_system_message
            )
            
            try:
                match = re.search(r"```(json)?\s*([\s\S]*?)\s*```", full_response_content)
                if match:
                    json_str = match.group(2)
                else:
                    json_str = full_response_content.strip()

                generated_tcs = json.loads(json_str)
                for tc in generated_tcs:
                    tc_id = f"TC-{tc_id_counter:03d}"
                    tc['id'] = tc_id
                    final_tcs_to_write[tc_id] = tc
                    tc_id_counter += 1
                    title_display = tc.get('title', 'No Title')
                    if not title_display.strip():
                        title_display = "(Empty Title)"
                    print(f"    - {tc['id']}: {title_display}")
                print(f"âœ… Generated {len(generated_tcs)} TCs for {srs_item.get('ID')}.")
                create_tc_excel(list(final_tcs_to_write.values()), output)
                print(f"ğŸ“„ Progress saved to {output}")
            except json.JSONDecodeError as e:
                print(f"âŒ Failed to parse JSON response for {srs_item.get('ID')}: {e}")
                print(f"Raw response: {full_response_content[:500]}...")

    except Exception as e:
        print(f"âŒ An error occurred during generation: {e}")
        raise typer.Exit(code=1)
    finally:
        rt.stop()


@app.command()
def update(
    excel_file: Annotated[str, typer.Argument(help="The path to the Excel file containing test cases.")],
    tc_id: Annotated[str, typer.Argument(help="The ID of the test case to update.")],
):
    """Updates a specific test case in the Excel file using LLM."""
    print(f"ğŸ” Attempting to update TC '{tc_id}' in '{excel_file}'...")

    # 1. Load Excel and find the TC
    all_tcs = read_tc_excel(excel_file)
    if not all_tcs:
        print(f"âŒ Error: No test cases found in {excel_file} or file not found.")
        raise typer.Exit(code=1)

    original_tc = None
    original_tc_index = -1
    for i, tc in enumerate(all_tcs):
        if tc.get("id") == tc_id:
            original_tc = tc
            original_tc_index = i
            break

    if not original_tc:
        print(f"âŒ Error: Test case with ID '{tc_id}' not found in {excel_file}.")
        raise typer.Exit(code=1)

    print("\n--- Original Test Case ---")
    print(json.dumps(original_tc, indent=2, ensure_ascii=False))

    # 2. Prompt LLM for Update
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ Error: GEMINI_API_KEY environment variable not set.")
        raise typer.Exit(code=1)

    rt = Runtime()
    try:
        system_message = UPDATE_SYSTEM_MESSAGE
        
        current_tc_for_llm = original_tc # This will be the TC that the LLM sees and modifies                
        while True: # Loop until user confirms or cancels
            update_instruction = typer.prompt("ì–´ë–»ê²Œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìˆ˜ì •í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? (ì˜ˆ: 'ìŒì„± ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€', 'ì‚¬ì „ ì¡°ê±´ ëª…í™•í™”', 'no' ì…ë ¥ ì‹œ ì¢…ë£Œ)")
            
            if update_instruction.lower() == "no":
                print("âŒ Update cancelled by user. No changes applied.")
                break

            llm_query_prompt = f"""
            ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì‚¬ìš©ì ì§€ì‹œì— ë”°ë¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
            --- í˜„ì¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ---
            {json.dumps(current_tc_for_llm, indent=2, ensure_ascii=False)}

            --- ì‚¬ìš©ì ì§€ì‹œ ---
            {update_instruction}

            ìˆ˜ì •ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìœ„ì—ì„œ ì œì‹œëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
            """

            print(f"ğŸ¤– Requesting LLM ({_current_llm_model_id}) to update test case...")
            full_response_content = _call_llm_with_fallback(
                rt=rt,
                api_key=api_key,
                prompt=llm_query_prompt,
                system_message=system_message
            )
            
            # LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì²˜ë¦¬)
            match = re.search(r"```(json)?\s*([\s\S]*?)\s*```", full_response_content)
            if match:
                json_str = match.group(2)
            else:
                json_str = full_response_content.strip()
            
            try:
                updated_tc = json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"âŒ Failed to parse JSON response from LLM: {e}")
                print(f"Raw LLM response: {full_response_content[:500]}...")
                print("âš ï¸ Please try again with a different instruction or type 'no' to exit.")
                continue # Continue the loop to ask for new instruction
            
            # 3. Display Proposed Changes (simple diff for now)
            print("\n--- Proposed Updated Test Case ---")
            print(json.dumps(updated_tc, indent=2, ensure_ascii=False))
            
            # 4. User Confirmation
            confirmation = typer.prompt("ì´ ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ('yes' ì…ë ¥ ì‹œ ì ìš©, ë‹¤ë¥¸ ì§€ì‹œ ì…ë ¥ ì‹œ LLM ì¬ìš”ì²­, 'no' ì…ë ¥ ì‹œ ì¢…ë£Œ)")
            
            if confirmation.lower() == "yes":
                all_tcs[original_tc_index] = updated_tc
                create_tc_excel(all_tcs, excel_file)
                print(f"âœ… Test case '{tc_id}' successfully updated in {excel_file}.")
                break # Exit loop after successful update
            elif confirmation.lower() == "no":
                print("âŒ Update cancelled by user. No changes applied.")
                break # Exit loop if user cancels
            else:
                # If not 'yes' and not 'no', treat it as a new instruction for the LLM
                # The next iteration of the loop will use this as the new update_instruction
                # And the LLM will modify the 'updated_tc' from this iteration.
                # So, we need to make 'updated_tc' the 'current_tc_for_llm' for the next iteration.
                current_tc_for_llm = updated_tc
                # update_instruction = confirmation # This line is not needed as the loop will re-prompt for instruction
                print("\nğŸ”„ Re-prompting LLM with your new instruction...")
    except Exception as e:
        print(f"âŒ An error occurred during update: {e}")
        raise typer.Exit(code=1)
    finally:
        rt.stop()

    print("âœ¨ Test case update process completed.")

if __name__ == "__main__":
    app()
